---
title: "Challenge B - Solution key"
author: "Oscar Jara - Nicolas Martinez"
date: "08/12/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Uploading packages, echo=FALSE, message=FALSE, warning=FALSE}
#We first upload the packages that we will need.
library(np)
library(tidyverse)
library(caret)
library(dplyr)  
library(tibble)
library(Metrics)
library(randomForest)
library(data.table)
library(stringr)
library(knitr)
```
The route of the repo is the following: XXXXXXXXXXXXXXXXXX


## Task 1B - Predicting house prices in Ames, Iowa (continued)

Step 1: The Random Forest algorythm works creating a large collection of correlated decision trees. Is a method that increases classification accurary with unbiassed and less noisy models to create a model with low variance.The term "forest" depicts a situation where a lot of decision trees are used. Particularly, the Random Forest method takes a training sample from which it creates subsamples. From them, this tool also generates random subsamples to produce decision trees. This decision trees represent variations of the the main clasification. With them it is possible to get  a ranking of clasifiers. With these ones, it is possible to make predictions.

Step 2: We train the Random Forest model in the training data. First, the data is corrected for the presence of missing data and factor variables. The variables that had a greater significance in the last project are used to asses.
```{r housing-step1-uploading data, include=FALSE, message=FALSE, warning=FALSE}
train <- read_csv(file = "data/raw-data/train.csv")
test <- read_csv(file = "data/raw-data/test.csv") 
```

```{r housing-step2-convert charct to fact, include=FALSE, message=FALSE, warning=FALSE}
#Convert character to factors 
train=train %>% mutate_if(is.character, as.factor)
test=test %>% mutate_if(is.character, as.factor)
```

```{r missing data, include=FALSE, message=FALSE, warning=FALSE}
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations")  %>%  filter( missing.observations > 0)
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
train <- train %>% select(- one_of(remove.vars))
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
```

```{r housing-step1-fitting a regre, echo=FALSE, message=FALSE, warning=FALSE}
# We run the model.
set.seed(123)
modelRF<-randomForest(SalePrice ~ LotArea+HouseStyle+OverallQual+
 OverallCond+YearBuilt+KitchenQual+Fireplaces+BsmtQual+GarageCars,
 data = train,mtry=2,ntree=1000)
```

Step 3: Using the model employed in the step 2, we make predictions for the test set (found in test.csv). We export the predictions to the csv file "predictionsRFF.csv". We also run an OLS model and make a prediction of the sale price using its inputs. Comparing both results, we can observe that the one made with the Random Forest technique possess a better fit. The prediction made by the OLS model has a low level of variation in its sale prices, situation that is different from the one showed by the training data. The Random Forest model has a better result capturing this variance.
```{r housing-step11-sol, echo=FALSE, message=FALSE, warning=FALSE}
#First, predictions for the Random Forest model are done. 
predictionRF<-predict(modelRF, mewdata=test, type="response")
write.csv(x = predictionRF, file = "predictionsRF.csv", na = "NA", quote = FALSE, row.names = FALSE)
#We run an alternative least square model using the same variables. 
modelOLS <- lm(SalePrice ~ LotArea+HouseStyle+OverallQual+OverallCond+YearBuilt+KitchenQual+Fireplaces+BsmtQual+GarageCars, data = train)
#We predict using the least squares model
predictionOLS <- data.frame(Id = test$Id, SalePrice_predict = predict(modelOLS, test, type="response"))
write.csv(x = predictionOLS, file = "predictionsOLS.csv", na = "NA", quote = FALSE, row.names = FALSE)

detach("package:randomForest", unload=TRUE)
```

## Task 2B - Overfitting in Machine Learning (continued)

Step 1: First we create the data using the same code employed in challenge A.

```{r Creating data to performing model, message=FALSE, include=FALSE}

#First we set the seed to 1 in order to compare the results.
set.seed(1)

#Second we create the vectors with random draws of the normal distribution for x and epsilon.
Nsim <- 150
b <- c(0,1)
x <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X0 <- cbind(x, x1^3)
y.true <- X0 %*% b

eps <- rnorm(n = Nsim)
y <- X0 %*% b + eps

#Finally, we create a dataframe with the vectors needed for the exercice, the values of x and y.
dataset <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)

#First, we had a column with the index of each observation (from 1 to 150).
dataset$observation <- 1:nrow(dataset)

# Split sample into training and testing, 80/20
training.index <- createDataPartition(y = y, times = 1, p = 0.8)
dataset <- dataset %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "testing"))

training <- dataset %>% filter(which.data == "training")
testing <- dataset %>% filter(which.data == "testing")

```
Then we proceed to run the local linear model on the training data.

```{r Estimating the low-flexibility model, include=FALSE}
ll.fit.lowflex <- npreg(y ~ x, bws = 0.5, data = training, method = "ll")
summary(ll.fit.lowflex)

```

Step 2: We run the high-flexibility local linear model.
```{r Estimating the high-flexibility model, include=FALSE}
ll.fit.highflex <- npreg(y ~ x, bws = 0.01, data = training, method = "ll")
```

Step 3: First we compute the predictions (fitted values) for both models.Then, we plot the different estimations, the true line, amd the scatter plot.

```{r Plotting the predictions, echo=FALSE}
#First we create the predictions for both models and we add them to training
training <- training %>% mutate(y.lfit.lwf = predict(object = ll.fit.lowflex))
training <- training %>% mutate(y.lfit.hgf = predict(object = ll.fit.highflex))

#Now we plot all the curves in the same plot
fig1 <- ggplot(training) + geom_point(mapping = aes(x = x, y = y)) + 
  ggtitle("Figure 1: Step 3 - Predictions of ll.fit.lowflex and ll.fit.highflex on training data") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.lfit.lwf), color = "red") + 
  geom_line(mapping = aes(x = x, y = y.lfit.hgf), color = "blue") 
```

Step 4: We compute the variance for each prediction and compare to each other.

```{r Comparng the variance of each estimation, include=FALSE}
#We compute the variance for each prediction
var(training$y.lfit.lwf)
var(training$y.lfit.hgf)

#we compare the variance of both predictions
var(training$y.lfit.lwf) <  var(training$y.lfit.hgf)
```
The model with the lowest variance is the low-flexibility local linear model (with a variance equal to 2.3272 versus a variance of 7.09).

To compare the bias of the models, we first compute the absolute value of the bias for each prediction. Then we sum those values for each model. This way we have a way to quantify which model has the biggest bias.

```{r Comparing the bias of each estimation method, include=FALSE}
#First we compute the absolute value of the bias of each prediction and add it as colum to the training data frame
training <- training %>% mutate(lwf.bias = abs(y - y.lfit.lwf))
training <- training %>% mutate(hgf.bias = abs(y - y.lfit.hgf))

#We compare the sum of the absolute values of the residuals to chech which one is higher
sum(training$lwf.bias) > sum(training$hgf.bias)
```
After computing those values in R, we find that the model with the lowest bias is the high-flexibility local linear model.


Step 5: First we proceed to run the models in the test data and to plot them.

```{r Plotting the predictions using test data, echo=FALSE}
#ll.fit.lowflex2 <- npreg(bws = 0.5, ...=11, y ~ x, xdat=testing$x, ydat = testing$y)
#ll.fit.highflex2 <- npreg(bws = 0.01, ...=11, y ~ x, xdat=testing$x, ydat = testing$y)

#we add columns with the predicted value for each model to the test dataframe
testing <- testing %>% mutate(y.lfit.lwf = predict(object = ll.fit.lowflex, newdata=testing))
testing <- testing %>% mutate(y.lfit.hgf = predict(object = ll.fit.highflex, newdata=testing))

fig2 <- ggplot(testing) + geom_point(mapping = aes(x = x, y = y)) + 
  ggtitle("Figure 2: Step 5 - Predictions of ll.fit.lowflex2 and ll.fit.highflex2 on testing data") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.lfit.lwf), color = "red") + 
  geom_line(mapping = aes(x = x, y = y.lfit.hgf), color = "blue") 
```
We proceed to compare the variance of the two models using the test data. Then, we compare the variance of each type of model under the different datasets.

```{r Comparing the variance of each estimation using test data, include=FALSE}
#We compute the variance for each prediction
var(testing$y.lfit.lwf)
var(testing$y.lfit.hgf)

#we compare the variance of both predictions
var(testing$y.lfit.lwf) <  var(testing$y.lfit.hgf)
var(testing$y.lfit.lwf) <  var(training$y.lfit.lwf)
var(testing$y.lfit.hgf) <  var(training$y.lfit.hgf)

```

Comparing the variance of the predictions in the different models, we see that, under both datasets, the low-flexibility local linear model has a lower variance than the high-flexibility local linear model. 

We then compare each type of model predictions under the different datasets. We find that for both models, the predictions have a  higher variance for the models estimated using the training data. This could be related to the size of the sample, as the test data has less observations than the training one.


```{r Comparing the bias of each estimation method using test data, include=FALSE}
#First we compute the absolute value of the bias of each prediction and add it as colum to the training data frame
testing <- testing %>% mutate(lwf.bias = abs(y - y.lfit.lwf))
testing <- testing %>% mutate(hgf.bias = abs(y - y.lfit.hgf))

#We compare the sum of the absolute values of the residuals to chech which one is higher
sum(testing$lwf.bias) < sum(testing$hgf.bias)
sum(testing$lwf.bias) < sum(training$lwf.bias)
sum(testing$hgf.bias) < sum(training$hgf.bias)
```
On the test data, the high-flexibility model is the one with the highest bias. Comparing estimations witht the two different datasets we find that the models have a higher bias in the training sample. 

Step 6: The step is performed and commented in the code.
```{r Computing the bandwidth vector,  include=FALSE}
#this vector gives you all the bandwidth values
bndwth <- c(seq(0.01, 0.5, 0.001))
#we create a new dataset in advance for the step 10
dataset2 <- data.frame(bndwth)
```

Step 7 and 8: The steps are performed and commented in the code.
```{r Computing the MSE for the training data,  include=FALSE}
#function that performs the estimation with a bandwidth parameter = i
ll.fit.trai.i <- function(i){
  npreg(y ~ x, bws = i, data = training, method = "ll")  
}
#function that computes the mean squared error of ll.fit.trai.i for a given value of i
MSE.ll.fit.trai.i <- function(n){
  mse(training$y, predict(object = ll.fit.trai.i(n)))
}
#We use mutate and sappply to create a vector with the MSE for all bandwidth values
dataset2 <- dataset2 %>% mutate(MSE.trai = sapply(X = bndwth , FUN = MSE.ll.fit.trai.i))
```

Step 9: We create a vector that stores the information for all the regressions done on the training data. Then we use that vector to compute the predictions of each one of those regressions over the test data. Finally, we use mutate and sappply to create a vector with the MSE for all bandwidth values, for the test database.

```{r Computing the MSE of the estimations on the test data, include=FALSE}
#first we create a vector that stores the information for all the regressions done on the training data
ll.fit.training.bdw <- lapply(X=bndwth, FUN= function(bndwth){
  npreg(y ~ x, bws = bndwth, data = training, method = "ll")  
})
#we use that vector to compute the predictions of each one of those regressions over the test data
ll.fit.test.bdw <- predict(object = ll.fit.training.bdw, newdata=testing)
#We use mutate and sappply to create a vector with the MSE for all bandwidth values, for the test data
dataset2 <- dataset2 %>% mutate(MSE.test = sapply(X = ll.fit.test.bdw, FUN= function(a){mean((a - testing$y)^2)}))
```


Step 10: The graph created summarizes the relationship between the MSE and the bandwidth under both datasets (in orange the test data and in blue the training data). The MSE is an increasing function of the bandwidth for the training data. Under the test data, the MSE reaches a minimum in a bandwidth value of 0.25.

```{r Ploting the relation between bandwidth and MSE, echo=FALSE}
fig3 <- ggplot(dataset2) +
  ggtitle("Figure 3: Step 10 - MSE on training and test data for different bandwidth - local linear regression") + 
  theme(plot.title = element_text(hjust = 0.5))+ 
  geom_line(mapping = aes(x = bndwth, y = MSE.test), color = "orange") +
  geom_line(mapping = aes(x = bndwth, y = MSE.trai), color = "blue")  
```


## Task 3B - Privacy regulation compliance in France
Step 1: We import the data of companies or organization in France wishing to adopt the regulatory framework of the CNIL. This data is directly imported from the Open Data Portal link page.We employ the command "fread".
```{r regulation-step1, echo=FALSE, message=FALSE, warning=FALSE}
dataCIL <- fread('https://www.data.gouv.fr/s/resources/correspondants-informatique-et-libertes-cil/20171115-183631/OpenCNIL_Organismes_avec_CIL_VD_20171115.csv')
attach(dataCIL)
```
Step 2: A table that contains the number of companies/organizations for each department in france is made with the kable and aggregate command. We use a unique count technique because in the CNIL database there firms with more than CIL offircer, so they are in more than two rows.
```{r regulation-step2, echo=FALSE, message=FALSE, warning=FALSE}
table3.2<-kable(aggregate(Siren ~ str_sub(Code_Postal,start = 1,end = 2), dataCIL, function(x) length(unique(x))), caption="number of organizations that has nominated a CNIL per department", col.names = c("Department","Number of firms"), format.args = list( decimal.mark = ","))
```

Step 3: Once downloaded the CNIL dataset, we move forward to the merge process. In the firstplace, we obtain the unique Siren codes from this data set.Then, we import the data from the CNIL and create a loop to extract some portions. We delimit this procedure for some parts of the Siren database. FOr instance, we show the results for the 10% of the data, in order to fully run this entire file.
```{r regulation-step3 part1, echo=FALSE, message=FALSE, warning=FALSE}
id<-dataCIL[,1] # We extract the Siren codes from the CIl database.
id<-id[!duplicated(id), ] #Keep only unique values
varnames<-c("Siren", "NIC", "L1_NORMALISEE", "L2_NORMALISEE", "L3_NORMALISEE" , "L4_NORMALISEE", "L5_NORMALISEE", "L6_NORMALISEE", "L7_NORMALISEE", "L1_DECLAREE", "L2_DECLAREE", "L3_DECLAREE","L4_DECLAREE", "L5_DECLAREE"  , "L6_DECLAREE" , "L7_DECLAREE" , "NUMVOIE" , "INDREP", "TYPVOIE", "LIBVOIE", "CODPOS","CEDEX", "RPET", "LIBREG", "DEPET", "ARRONET", "CTONET","COMET","LIBCOM","DU", "TU", "UU", "EPCI","TCD", "ZEMET", "SIEGE","ENSEIGNE", "IND_PUBLIPO", "DIFFCOM", "AMINTRET",  "NATETAB", "LIBNATETAB", "APET700", "LIBAPET","DAPET", "TEFET", "LIBTEFET", "EFETCENT", "DEFET", "ORIGINE", "DCRET", "DDEBACT", "ACTIVNAT","LIEUACT","ACTISURF", "SAISONAT", "MODET", "PRODET", "PRODPART", "AUXILT", "NOMEN_LONG","SIGLE", "NOM", "PRENOM", "CIVILITE", "RNA", "NICSIEGE", "RPEN", "DEPCOMEN", "ADR_MAIL", "NJ","LIBNJ", "APEN700", "LIBAPEN", "DAPEN", "APRM", "ESS", "DATEESS", "TEFEN", "LIBTEFEN", "EFENCENT", "DEFEN", "CATEGORIE", "DCREN", "AMINTREN" , "MONOACT", "MODEN", "PRODEN", "ESAANN", "TCA", "ESAAPEN", "ESASEC1N", "ESASEC2N", "ESASEC3N", "ESASEC4N", "VMAJ", "VMAJ1", "VMAJ2", "VMAJ3", "DATEMAJ")

for (i in 1:10){
if (i==1) {
dataSiren <- read.delim(file = "data/raw-data/sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv",sep = ";",     header = TRUE,nrows = 10000) #import just a fraction of the SIREN database.
names(dataSiren)[1]<-paste("Siren")
data1<-merge(dataSiren,id,by=c("Siren"),all =FALSE) #merge with the one of CIL.
} else {
dataSiren <- read.delim(file = "data/raw-data/sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv",sep = ";", header = FALSE,skip=10000*(i-1), nrows = 10000)  #import just a fraction of the SIREN database.
dataSiren<-setNames(dataSiren,varnames) #assign names to the columns.
data2<-merge(id,dataSiren,by=c("Siren"),all =FALSE) # look for siren codes that coincide.
colnames(data2)<-NULL #delete the column names
data1<-rbind(data1,data2) #append the new dataset to the one alredy created.
}
}
data1<-setDT(data1)[,.SD[which.max(str_sub(DATEMAJ,start = 1,end = 4))],keyby=Siren] 


```

```{r full step3 part1, echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
#Complete chun
dataextraction<-function(x){
for (i in 1:15){
if (i==1) {
dataSiren <- read.delim(file = "data/raw-data/sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv",sep = ";",     header = TRUE,nrows = 10000) #import just a fraction of the SIREN database.
names(dataSiren)[1]<-paste("Siren")
data1<-merge(dataSiren,id,by=c("Siren"),all =FALSE) #merge with the one of CIL.
} else {
dataSiren <- read.delim(file = "data/raw-data/sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv",sep = ";", header = FALSE,skip=10000*(i-1), nrows = 10000)  #import just a fraction of the SIREN database.
dataSiren<-setNames(dataSiren,varnames) #assign names to the columns.
data2<-merge(id,dataSiren,by=c("Siren"),all =FALSE) # look for siren codes that coincide.
colnames(data2)<-NULL #delete the column names
data1<-rbind(data1,data2) #append the new dataset to the one alredy created.
}
}
for (i in 16:59){
dataSiren <- read.delim(file = "data/raw-data/sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv",sep = ";", header = FALSE,skip=10000*(i-1), nrows = 10000)
dataSiren<-setNames(dataSiren,varnames)
data2<-merge(id,dataSiren,by=c("Siren"),all =FALSE)
colnames(data2)<-NULL
data1<-rbind(data1,data2)
}
for (i in 60:101){
dataSiren <- read.delim(file = "data/raw-data/sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv",sep = ";", header = FALSE,skip=10000*(i-1), nrows = 10000)
dataSiren<-setNames(dataSiren,varnames)
data2<-merge(id,dataSiren,by=c("Siren"),all =FALSE)
colnames(data2)<-NULL
data1<-rbind(data1,data2)
}
data1<-setDT(data1)[,.SD[which.max(str_sub(DATEMAJ,start = 1,end = 4))],keyby=Siren] 
}

system.time(dataextraction(5))
```



Step 4: We create a histogram of the size of the firms. We use the number of employees (variable EFENCENT) as a proxy of the size of the organization.
```{r regulation-step4, include=FALSE, message=FALSE, warning=FALSE}
replace(data1$EFENCENT,data1$EFENCENT=="NN","NA")
data1$EFENCENT<-as.integer(data1$EFENCENT)
hist3.4<-qplot(data1$EFENCENT,geom="histogram", binwidth = 70, main = "Histogram of EFENCENT",xlab = "Number of Employees",fill=I("blue"), col=I("red"), alpha=I(.2), xlim = c(0,1500))
```

## Anexes
Table from Tas 2B - step3
```{r fig1, echo=TRUE, message=FALSE, warning=FALSE}
fig1
```
Table from Tas 2B - step5
```{r fig2, echo=TRUE, message=FALSE, warning=FALSE}
fig2
```
Table from Tas 2B - Step10
```{r fig3, echo=TRUE, message=FALSE, warning=FALSE}
fig3
```

Table from Tas 3B - Step2
```{r table, echo=TRUE, message=FALSE, warning=FALSE}
table3.2
```
Histogram from Task 3B - Step4
```{r histogram, echo=TRUE, message=FALSE, warning=FALSE}
hist3.4
```